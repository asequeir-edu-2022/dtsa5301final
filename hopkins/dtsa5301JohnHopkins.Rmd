---
title: "COVID19 Data from John Hopkins"
author: "Antony Sequeira"
date: "2/9/2022"
output: pdf_document
---

# Following script installs the required libraries in Mac OSX
This section can be copied to a file or input into R console.  
You could also download it from my repo at https://github.com/asequeir-edu-2022/dtsa5301final

```
#!/usr/bin/env Rscript
r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"
options(repos = r)

print("Installing R libraries")
install.packages("chron")
install.packages("tidyverse")
install.packages("tinytex")

tinytex::install_tinytex()
```

# Overview
For fulfillment of DTSA-5301 finals *COVID19 dataset from the Johns Hopkins github site* part of the assignment.

# TODO explain data source
# TODO explain data
# TODO clean up data
# TODO create various ARDs
# TODO create plots (2)
# TODO model (1)
# TODO bias

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(lubridate)
library(chron)
```
# Data source
The main data source is the github repository at
https://github.com/CSSEGISandData/COVID-19

We will use the data from the folder at  
https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series

Two of the time series tables are for the US confirmed cases and deaths, reported at the county level.  
They are `time_series_covid19_confirmed_US.csv` and `time_series_covid19_deaths_US.csv` respectively.

These two files will provide our data for the analysis.

```{r load_data, echo=TRUE, results=FALSE, message=FALSE}
covid19_confirmed_url = "https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv"
covid19_deaths_url = "https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv"

covid19_confirmed_raw <- read_csv(covid19_confirmed_url)
covid19_deaths_raw <- read_csv(covid19_deaths_url)
```


## Clean up data
We clean up the data mainly through the following three operations:  
- variables to factor for appropriate columns  
- date types from strings  
- remove unneeded columns  
- pivot

```{r clean_data, echo=TRUE}
covid19_confirmed <- covid19_confirmed_raw %>%
  pivot_longer(cols = -c("UID", "iso2", "iso3", "code3", "FIPS", "Admin2",
                         "Province_State","Country_Region", "Lat", "Long_", "Combined_Key"),
                       names_to = "date",
                       values_to = "cases") %>%
  select(-c(Lat, Long_)) ## unwanted columns?

```

```{r clean_data, echo=TRUE}

ny_data <- ny_data_raw %>%
  mutate(OCCUR_DATE = mdy(OCCUR_DATE)) %>%
  mutate(OCCUR_TIME = chron(times=OCCUR_TIME)) %>%
  mutate(BORO = factor(BORO)) %>%
  mutate(PRECINCT = factor(PRECINCT)) %>%
  mutate(PERP_AGE_GROUP = factor(PERP_AGE_GROUP)) %>%
  mutate(PERP_SEX = factor(PERP_SEX)) %>%
  mutate(PERP_RACE = factor(PERP_RACE)) %>%
  mutate(VIC_AGE_GROUP = factor(VIC_AGE_GROUP)) %>%
  mutate(VIC_SEX = factor(VIC_SEX)) %>%
  mutate(VIC_RACE = factor(VIC_RACE)) %>%
  select (-c(JURISDICTION_CODE, LOCATION_DESC, X_COORD_CD, Y_COORD_CD, Latitude, Longitude, INCIDENT_KEY))
```

## Missing data columns and plans to handle them
There is missing data in the following columns:
```{r missing_data, echo=TRUE}
names(which(colSums(is.na(ny_data_raw)) > 0))
```
Missing data in factor columns `PERP_SEX` etc. are handled already as a factor.  
I do not plan to use `JURISDICTION_CODE` and `LOCATION_DESC`.  
So, for this data, nothing more needs to be done for missing data handling. 

### Summary of the cleaned up data

```{r summary, echo=TRUE}
summary(ny_data)
```
Turning columns into factors shows total counts for columns such as `BORO`.  
The summary also shows the breakdown of total incidents by factor type columns such as `PERP_AGE_GROUP`.  


## Visualization and Analysis

### Generate necessary analysis ready data.  
Prepare a few different slices of the data for visualizations.  

```{r try1}
ny_data_sum <- ny_data %>%
  group_by(BORO, VIC_AGE_GROUP) %>% 
  mutate(BORO_BY_VIC_AGE = n()) %>%  # occurrences by victim age group by boro
  ungroup()  %>%
  group_by(month = lubridate::floor_date(OCCUR_DATE, "month")) %>%
  mutate(month_sum = sum(n())) %>% # occurrences by month
  ungroup() %>%
  group_by(month, BORO) %>%
  mutate(month_by_boro = sum(n())) %>%
  ungroup()
```

```{r sum2}
ny_data_sum2 <- ny_data %>%
  group_by(BORO, VIC_AGE_GROUP) %>% 
  summarize(BORO_BY_VAG = n(), .groups = 'drop') %>%
  ungroup()
```

```{r sum3}
ny_data_sum3 <- ny_data %>%
  group_by(month = lubridate::floor_date(OCCUR_DATE, "month")) %>%
  summarise(month_sum = sum(n())) %>% # occurrences by month
  ungroup()
```

## Visualizations
### Variations by the month of the year
We plot the total incidents by month.  
```{r plot1}
ny_data_sum3 %>%
ggplot(aes(x = month, y = month_sum)) +
geom_line(aes(color = "month")) +
geom_point(aes(color = "month")) +
geom_line(aes(y=month_sum, color = "month_sum")) +
geom_point(aes(y=month_sum, color = "month_sum")) +
labs(title = "Incidents in NY by month", y = NULL)
```
The above plot shows that the number of incidents vary seasonally.  
To see the 2020 peak more clearly, we plot a shorter time span.  
```{r plot2}
ny_data_sum3 %>%
  filter(year(month) > 2019) %>% 
  ggplot(aes(x = month, y = month_sum)) +
  geom_line(aes(color = "month")) +
  geom_point(aes(color = "month")) +
  geom_line(aes(y=month_sum, color = "month_sum")) +
  geom_point(aes(y=month_sum, color = "month_sum")) +
  labs(title = "Incidents in NY by month for years 2019 onwards", y = NULL)
```

This shows a clear peak in July of 2020.  

### Variations by borough
Generate the counts by borough.  
```{r plot4}
ny_data_sum %>%
  group_by(month_by_boro) %>%
  ggplot(aes(x=month, y=month_by_boro, group=BORO, color=BORO)) +
  geom_line() + 
  labs(title = "Incidents in NY by month by Boro", y = NULL)
```

Get a smaller time span to see a zoomed in view.  
```{r plot5}
ny_data_sum %>%
  filter(year(month) > 2015) %>% 
  filter(year(month) < 2019) %>%
  group_by(month_by_boro) %>%
  ggplot(aes(x=month, y=month_by_boro, group=BORO, color=BORO)) +
  geom_line() + 
  labs(title = "Plot fewer years to show peaks", y = NULL)
```

## Analysis
The plots show the following:  

- seasonal peaks mostly in summer
- higher levels of incidents based on the boro - Staten Island is lowest and Bronx and Brooklyn seem to be the higher end.
- the incidents show unusual higher numbers in first quarter of 2020


## Questions raised by the visualization and analysis (to be investigated)
- population of boros (Staten Island might have much smaller population) may be too different
- factors not in data such as income
- number of police officers per person

## Bias
There could be multiple sources of bias in the NY COVID19 data

- the data collection may be biased, it is possible that not all COVID19s are reported
- the standard race categories may not reflect the reality of the John Hopkins demographics

I have tried to focus on the boro and seasonality of the data to reduce bias.

## Modeling
```{r}
ny_data_doy <- ny_data_sum %>%
  filter(year(OCCUR_DATE)< 2020) %>% # avoiding covid years
  group_by(doy = yday(OCCUR_DATE)) %>%
  mutate(doy_sum = sum(n())) %>%
  ungroup()

mod <- lm(doy_sum ~ doy, data=ny_data_doy)
summary(mod)

ny_data_pred <-  ny_data_doy %>% mutate(doy_sum_pred = predict(mod))

ny_data_pred %>% ggplot() +
  geom_point(aes( x = doy, y = doy_sum), color = "blue") +
  # compare with predicted
  geom_point(aes( x = doy, y = doy_sum_pred), color = "red")

```


The plot clearly shows that the linear model does not fit the data of incidents given a day of the year.  
It looks like we need a different model (other than linear) to predict incidents given any day of the year. 

I hope to learn more about statistical modelling in future data science courses so I can model such data better.

## Session info
```{r, echo=FALSE}
sessionInfo()
```
